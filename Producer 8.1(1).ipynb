{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import threading\n",
    "from pathlib import Path\n",
    "import time\n",
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=True,\n",
    "    client_kwargs={\n",
    "        'endpoint_url': endpoint_url\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Supraja',\n",
       " 'last_name': 'Rapuru',\n",
       " 'client_id': 'RapuruSupraja',\n",
       " 'topic_prefix': 'RapuruSupraja'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Supraja',\n",
    "    last_name='Rapuru'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadParquet(parq_path):\n",
    "    pqr = spark.read.parquet(parq_path)\n",
    "    # Convert from spark dataframe to pandas dataframe\n",
    "    pqr = pqr.toPandas()\n",
    "    return pqr\n",
    "\n",
    "def splitstr(std):\n",
    "    before, after = str(std).split('.')\n",
    "    return before, after\n",
    "\n",
    "def startTimer(results_dir):\n",
    "    # Loop on time\n",
    "    print(\"call function here\")\n",
    "    retval = startTimedParquetStreamUpdateLoop(results_dir)\n",
    "    # Stop if time is over and there are no more partitions.\n",
    "    if ((time.time() - start_time) < 70 and retval == 0):\n",
    "        t = threading.Timer(interval, startTimer(results_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic \"RapuruSupraja-locations\" already exists\n",
      "Topic \"RapuruSupraja-accelerations\" already exists\n"
     ]
    }
   ],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "    \n",
    "create_kafka_topic('locations')\n",
    "create_kafka_topic('accelerations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Producer\n",
    "\n",
    "The following code creates a `KafkaProducer` object which you can use to send Python objects that are serialized as JSON.\n",
    "\n",
    "**Note:** This producer serializes Python objects as JSON. This means that object must be JSON serializable.  As an example, Python `DateTime` values are not JSON serializable and must be converted to a string (e.g. ISO 8601) or a numeric value (e.g. a Unix timestamp) before being sent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(\n",
    "  bootstrap_servers=config['bootstrap_servers'],\n",
    "  value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Data Function\n",
    "\n",
    "The `send_data` function sends a Python object to a Kafka topic. This function adds the `topic_prefix` to the topic so `send_data('locations', data)` sends a JSON serialized message to `suprajarapuru-locations`. The function also registers callbacks to let you know if the message has been sent or if an error has occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_send_success(record_metadata):\n",
    "    print('Message sent:\\n    Topic: \"{}\"\\n    Partition: {}\\n    Offset: {}'.format(\n",
    "        record_metadata.topic,\n",
    "        record_metadata.partition,\n",
    "        record_metadata.offset\n",
    "    ))\n",
    "    \n",
    "def on_send_error(excp):\n",
    "    print('I am an errback', exc_info=excp)\n",
    "    # handle exception\n",
    "\n",
    "def send_data(topic, data, config=config, producer=producer, msg_key=None):\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    topic_name = '{}-{}'.format(topic_prefix, topic)\n",
    "    \n",
    "    if msg_key is not None:\n",
    "        key = msg_key\n",
    "    else:\n",
    "        key = uuid.uuid4().hex\n",
    "    \n",
    "    producer.send(\n",
    "        topic_name, \n",
    "        value=data,\n",
    "        key=key.encode('utf-8')\n",
    "    ).add_callback(on_send_success).add_errback(on_send_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message sent:\n",
      "    Topic: \"RapuruSupraja-locations\"\n",
      "    Partition: 0\n",
      "    Offset: 3\n"
     ]
    }
   ],
   "source": [
    "example_data = dict(\n",
    "    key1='value1',\n",
    "    key2='value2'\n",
    ")\n",
    "\n",
    "send_data('locations', example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceleration_columns = [\n",
    "    'offset',\n",
    "    'id',\n",
    "    'ride_id',\n",
    "    'uuid',\n",
    "    'x',\n",
    "    'y',\n",
    "    'z',\n",
    "     't'\n",
    "]\n",
    "Acceleration = namedtuple('Acceleration', acceleration_columns)\n",
    "\n",
    "df_acc = pq.ParquetDataset('s3://data/processed/bdd/accelerations',filesystem=s3).read_pandas().to_pandas()\n",
    "df_acc = df_acc[acceleration_columns].sort_values(by=['offset'])\n",
    "\n",
    "location_columns = [\n",
    "    'offset',\n",
    "    'id',\n",
    "    'ride_id',\n",
    "    'uuid',\n",
    "    'course',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'geohash',\n",
    "    'speed',\n",
    "    'accuracy',\n",
    "     't'\n",
    "]\n",
    "Location = namedtuple('Location', location_columns)\n",
    "\n",
    "df_loc = pq.ParquetDataset('s3://data/processed/bdd/locations',filesystem=s3).read_pandas().to_pandas()\n",
    "df_loc = df_loc[location_columns].sort_values(by=['offset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "offset = []\n",
    "topics = []\n",
    "datas = []\n",
    "for i in range(df_acc.shape[0]):\n",
    "    data_acc = df_acc.loc[i].to_dict()\n",
    "    offset.append(data_acc['offset'])\n",
    "    datas.append(data_acc)\n",
    "    topics.append('accelerations')\n",
    "for i in range(df_loc.shape[0]):\n",
    "    data_loc = df_loc.loc[i].to_dict()\n",
    "    offset.append(data_loc['offset'])\n",
    "    datas.append(data_loc)\n",
    "    topics.append('locations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offset</th>\n",
       "      <th>topics</th>\n",
       "      <th>datas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{'offset': 0.8220608865228429, 'id': '58682c5d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.842061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{'offset': 0.8420608865228429, 'id': '58682c5d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.862061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{'offset': 0.862060886522843, 'id': '58682c5d4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.882061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{'offset': 0.882060886522843, 'id': '58682c5d4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.902061</td>\n",
       "      <td>accelerations</td>\n",
       "      <td>{'offset': 0.9020608865228429, 'id': '58682c5d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     offset         topics                                              datas\n",
       "0  0.822061  accelerations  {'offset': 0.8220608865228429, 'id': '58682c5d...\n",
       "1  0.842061  accelerations  {'offset': 0.8420608865228429, 'id': '58682c5d...\n",
       "2  0.862061  accelerations  {'offset': 0.862060886522843, 'id': '58682c5d4...\n",
       "3  0.882061  accelerations  {'offset': 0.882060886522843, 'id': '58682c5d4...\n",
       "4  0.902061  accelerations  {'offset': 0.9020608865228429, 'id': '58682c5d..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining the lists into a dataframe and sorting by offset\n",
    "df = pd.DataFrame({'offset': offset, 'topics': topics, 'datas': datas})\n",
    "df = df.sort_values(by = ['offset'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    offset = df['offset'][i]\n",
    "    while (time.time() - start_time) < offset:\n",
    "        pass\n",
    "    send_data(df['topics'][i], df['datas'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
